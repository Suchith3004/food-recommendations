{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Convert Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert dataset from the following format to a CSV\n",
    "product/productId: B001E4KFG0\n",
    "review/userId: A3SGXH7AUHU8GW\n",
    "review/profileName: delmartian\n",
    "review/helpfulness: 1/1\n",
    "review/score: 5.0\n",
    "review/time: 1303862400\n",
    "review/summary: Good Quality Dog Food\n",
    "review/text: I have bought several of the Vitality canned dog food products and have\n",
    "found them all to be of good quality. The product looks more like a stew than a\n",
    "processed meat and it smells better. My Labrador is finicky and she appreciates this\n",
    "product better than most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://snap.stanford.edu/data/web-FineFoods.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gzip.open('finefoods.txt.gz', 'rb')\n",
    "\n",
    "df = pd.DataFrame() #dataframe to store reviews\n",
    "\n",
    "num_rows = 200000   #The file size is very large so limit the number of rows (approximately 1/3 of dataset)\n",
    "curr_row = 0     #Counter for current row\n",
    "\n",
    "currRow = {}\n",
    "\n",
    "for line in g:\n",
    "    try:\n",
    "        cleanedLine = line.decode(\"utf-8\").strip().split(': ', 1)\n",
    "        if(len(cleanedLine) == 1):\n",
    "            if('review/summary' in currRow and 'review/text' in currRow): #ensure the review is not empty\n",
    "                df = df.append(currRow, ignore_index=True)\n",
    "                curr_row += 1\n",
    "            currRow = {}\n",
    "        else:\n",
    "            #add to dataframe\n",
    "            currRow[cleanedLine[0]] = cleanedLine[1]\n",
    "    except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "        pass\n",
    "    if(num_rows == curr_row):\n",
    "        break\n",
    "\n",
    "#Rename headers\n",
    "df.rename(columns = { 'product/productId': 'productId',\n",
    "                    'review/helpfulness': 'helpfulness',\n",
    "                    'review/profileName': 'profileName',\n",
    "                    'review/score': 'score',\n",
    "                    'review/summary' : 'summary',\n",
    "                    'review/text' : 'text',\n",
    "                    'review/time' : 'time',\n",
    "                    'review/userId' : 'userId'}, inplace=True)\n",
    "\n",
    "#Store dataframe as csv\n",
    "df.to_csv('finefoods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>productId</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>profileName</th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>1/1</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>0/0</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   productId helpfulness profileName  score  \\\n",
       "0           0  B001E4KFG0         1/1  delmartian    5.0   \n",
       "1           1  B00813GRG4         0/0      dll pa    1.0   \n",
       "\n",
       "                 summary                                               text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "\n",
       "         time          userId  \n",
       "0  1303862400  A3SGXH7AUHU8GW  \n",
       "1  1346976000  A1D87F6ZCVE5NK  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read stored csv, rather than create it again\n",
    "df = pd.read_csv(\"finefoods.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer reccomendations using numerical ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://medium.com/@lope.ai/recommendation-systems-from-scratch-in-python-pytholabs-6946491e76c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_sampled = df.sample(20000) #small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary with new indexes\n",
    "ff_users = {k: v for v, k in enumerate(list(ff_sampled['userId'].unique()))}\n",
    "ff_products = {k: v for v, k in enumerate(list(ff_sampled['productId'].unique()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_num_users = len(ff_users)\n",
    "ff_num_products = len(ff_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data up into training data and testing data (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(ff_sampled, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a user vs product matrix that contains all ratings values\n",
    "data_matrix = np.zeros((ff_num_users, ff_num_products))\n",
    "for index, review in train_data.iterrows():\n",
    "    #find index equivalent of user and product\n",
    "    row = ff_users[review['userId']]\n",
    "    column = ff_products[review['productId']]\n",
    "    data_matrix[row, column] = review['score']\n",
    "    \n",
    "    \n",
    "#create a user vs product matrix that contains all ratings values\n",
    "test_data_matrix = np.zeros((ff_num_users, ff_num_products))\n",
    "for index, review in test_data.iterrows():\n",
    "    #find index equivalent of user and product\n",
    "    row = ff_users[review['userId']]\n",
    "    column = ff_products[review['productId']]\n",
    "    test_data_matrix[row, column] = review['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type='reviewer'):\n",
    "    \n",
    "    if type == 'reviewer':\n",
    "        mean_reviewer_rating = ratings.mean(axis=1).reshape(-1,1)\n",
    "        #We use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        \n",
    "        ratings_diff = (ratings - mean_reviewer_rating)\n",
    "        pred = mean_reviewer_rating + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    \n",
    "    elif type == 'product':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create item predictions using user and item similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prediction = predict(data_matrix, user_similarity, type='reviewer') #do it for each chunk\n",
    "item_prediction = predict(data_matrix, item_similarity, type='product') #do it for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17801, 7764)\n",
      "(17801, 7764)\n",
      "17801\n",
      "7764\n"
     ]
    }
   ],
   "source": [
    "#Check if the matrix shape matches originial dimensions \n",
    "print(user_prediction.shape)\n",
    "print(item_prediction.shape)\n",
    "print(ff_num_users)\n",
    "print(ff_num_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluvation through RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.361098010013852\n"
     ]
    }
   ],
   "source": [
    "#user-user predictions\n",
    "prediction = user_prediction[test_data_matrix.nonzero()].flatten()\n",
    "ground_truth = test_data_matrix[test_data_matrix.nonzero()].flatten()\n",
    "print(sqrt(mean_squared_error(prediction, ground_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.362368358916537\n"
     ]
    }
   ],
   "source": [
    "#item-item predictions\n",
    "prediction = item_prediction[test_data_matrix.nonzero()].flatten()\n",
    "ground_truth = test_data_matrix[test_data_matrix.nonzero()].flatten()\n",
    "print(sqrt(mean_squared_error(prediction, ground_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for getting item reccomendation for single user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(userId, type='user'):\n",
    "    #get user index in matrix\n",
    "    user_idx = ff_users[userId]\n",
    "\n",
    "    #determine prediction matrix\n",
    "    prediction_matrix = user_prediction if type == 'user' else item_prediction\n",
    "    \n",
    "    #Determine predictions based on user similarity\n",
    "    pred_dict = {list(ff_sampled['productId'].unique())[v]: k for v, k in enumerate(prediction_matrix[user_idx])}\n",
    "    \n",
    "    return pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User A2A6SB596ROUZA:\n",
      "\t1. B002QWP89S 0.012528889553904608\n",
      "\t2. B001EO5Q64 0.012023024125573068\n",
      "\t3. B002YJ0118 0.011798195046314627\n",
      "\t4. B0026RQTGE 0.010842671459466144\n",
      "\t5. B003GTR8IO 0.010674049650022324\n",
      "\t6. B0090X8IPM 0.010393013300949243\n",
      "\t7. B005ZBZLT4 0.009887147872617701\n",
      "\t8. B006N3IG4K 0.009830940602803078\n",
      "\t9. B007M83302 0.009830940602803075\n",
      "\t10. B007Y59HVM 0.009549904253729995\n"
     ]
    }
   ],
   "source": [
    "user_id = list(ff_users.keys())[0]\n",
    "pred_dict = get_predictions(user_id, type='user')\n",
    "    \n",
    "#Return item's predictions    \n",
    "item_predictions = sorted(pred_dict, key=pred_dict.get, reverse=True)\n",
    "\n",
    "print(\"User {}:\".format(user_id))\n",
    "for i in range(10): \n",
    "    print(\"\\t{}. {} {}\".format(i+1, item_predictions[i], pred_dict[item_predictions[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User A2A6SB596ROUZA:\n",
      "\t1. B005J6AGKY 0.0006449486284821693\n",
      "\t2. B001HTG3IQ 0.0006449486284821693\n",
      "\t3. B006ZGE2JS 0.0006449486284821693\n",
      "\t4. B00061KXUA 0.0006449486284821693\n",
      "\t5. B000GFZIN8 0.0006449486284821693\n",
      "\t6. B000LLS410 0.0006449486284821693\n",
      "\t7. B000EEWZC8 0.0006449486284821693\n",
      "\t8. B002CTAVK6 0.0006449333395300329\n",
      "\t9. B000F4ISDW 0.0006446738957901039\n",
      "\t10. B000FPKX5C 0.0006446487466219038\n"
     ]
    }
   ],
   "source": [
    "user_id = list(ff_users.keys())[0]\n",
    "pred_dict = get_predictions(user_id, type='item')\n",
    "    \n",
    "#Return item's predictions    \n",
    "item_predictions = sorted(pred_dict, key=pred_dict.get, reverse=True)\n",
    "\n",
    "print(\"User {}:\".format(user_id))\n",
    "for i in range(10): \n",
    "    print(\"\\t{}. {} {}\".format(i+1, item_predictions[i], pred_dict[item_predictions[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine average sentiment for recommended products "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senitment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine emoticons, hashtags, etc. to each term \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentiment(row):\n",
    "    cutoff = .5\n",
    "    if row['compound'] < -1 * cutoff:\n",
    "        return \"neg\"\n",
    "    elif row['compound'] > cutoff:\n",
    "        return \"pos\"\n",
    "    else:\n",
    "        return \"neu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_sentiment(text):\n",
    "    #Remove stopwords \n",
    "    sws = set(stopwords.words('english') + list(string.punctuation) + ['”', '’', '…', '‼', '‼‼', '“', 'i', 'i\\'m', 'the', 'they', '-', '<br />'])\n",
    "    text = ' '.join([i for i in tokens_re.findall(text) if i not in sws])\n",
    "    \n",
    "    #perform sentiment analysis\n",
    "    scores = sia.polarity_scores(text) #Get senitment analysis for the review\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct sentiment analysis on the summary of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_labels = []\n",
    "# summary_pos = []\n",
    "# summary_neg = []\n",
    "# summary_neu = []\n",
    "summary_compound = []\n",
    "for summary in df['summary']:\n",
    "    if summary == None or summary == 'nan' or isNaN(summary):\n",
    "        summary = ''\n",
    "    scores = determine_sentiment(summary)\n",
    "    label = label_sentiment(scores) #determine final sentiment\n",
    "    summary_labels.append(label)\n",
    "#     summary_pos.append(scores['pos'])\n",
    "#     summary_neg.append(scores['neg'])\n",
    "#     summary_neu.append(scores['neu'])\n",
    "    summary_compound.append(scores['compound'])\n",
    "\n",
    "# df['summary_pos'] = summary_pos\n",
    "# df['summary_neg'] = summary_neg\n",
    "# df['summary_neu'] = summary_neu\n",
    "df['summary_compound'] = summary_compound\n",
    "df['summary_label'] = summary_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct sentiment analysis on the full text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = []\n",
    "text_pos = []\n",
    "text_neg = []\n",
    "text_neu = []\n",
    "text_compound = []\n",
    "for full_text in df['text']:\n",
    "    scores = determine_sentiment(full_text)\n",
    "    label = label_sentiment(scores) #determine final sentiment\n",
    "    text_labels.append(label)\n",
    "    text_pos.append(scores['pos'])\n",
    "    text_neg.append(scores['neg'])\n",
    "    text_neu.append(scores['neu'])\n",
    "    text_compound.append(scores['compound'])\n",
    "\n",
    "df['text_pos'] = text_pos\n",
    "df['text_neg'] = text_neg\n",
    "df['text_neu'] = text_neu\n",
    "df['text_compound'] = text_compound\n",
    "df['text_label'] = text_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the average sentiment for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_product_list = df['productId'].unique()\n",
    "product_sentiment = pd.DataFrame()\n",
    "for product in ff_product_list:\n",
    "\n",
    "    product_reviews = df[df['productId'] == product]\n",
    "        \n",
    "    avg_summ_sentiment = product_reviews['summary_compound'].mean()\n",
    "    avg_text_sentiment = product_reviews['text_compound'].mean()\n",
    "    \n",
    "    summ_label = label_sentiment({'compound' : avg_summ_sentiment})\n",
    "    text_label = label_sentiment({'compound' : avg_text_sentiment})\n",
    "    \n",
    "    row = {'productId': product, \n",
    "                             'summ_compound': avg_summ_sentiment,\n",
    "                             'summ_label': summ_label,\n",
    "                             'text_compound' : avg_text_sentiment,\n",
    "                             'text_label' : text_label}\n",
    "    product_sentiment = product_sentiment.append(row , ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>summ_compound</th>\n",
       "      <th>summ_label</th>\n",
       "      <th>text_compound</th>\n",
       "      <th>text_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>0.440400</td>\n",
       "      <td>neu</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neu</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.807300</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neu</td>\n",
       "      <td>0.440400</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>0.602225</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.934475</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productId  summ_compound summ_label  text_compound text_label\n",
       "0  B001E4KFG0       0.440400        neu       0.941300        pos\n",
       "1  B00813GRG4       0.000000        neu       0.076200        neu\n",
       "2  B000LQOCH0       0.599400        pos       0.807300        pos\n",
       "3  B000UA0QIQ       0.000000        neu       0.440400        neu\n",
       "4  B006K2ZZ7K       0.602225        pos       0.934475        pos"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct Topic Modeling to determine topics for each product's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products(productId, topics, words):\n",
    "    product_reviews = df[df['productId'] == productId]\n",
    "    product_reviews = product_reviews['text']\n",
    "    \n",
    "    #Remove stopwords \n",
    "    sws = set(stopwords.words('english') + list(string.punctuation) + ['”', '’', '…', '‼', '‼‼', '“', 'i', 'the', 'they', '-', '<br />'])\n",
    "    sws_removed = []\n",
    "    for sent in product_reviews:\n",
    "        sws_removed.append([i for i in tokens_re.findall(sent.lower()) if i not in sws])\n",
    "        \n",
    "    #Create dictionary of the words in product reviews\n",
    "    dictionary = corpora.Dictionary(sws_removed)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.3)  #drop words that show up less than 5 times and in more than 30% of the tweets\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    #create bag of words from dictionary\n",
    "    corpus = [dictionary.doc2bow(text) for text in sws_removed]\n",
    "    \n",
    "    #create the LDA topic model\n",
    "    ldamodel = models.ldamodel.LdaModel(corpus, num_topics=topics, id2word=dictionary, passes=20)\n",
    "    \n",
    "    #print topics for reviews\n",
    "    for i, topic in ldamodel.print_topics(num_topics=3, num_words=5):\n",
    "        print(\"\\tTopic:{}  {}\".format(i, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTopic:0  0.022*\"price\" + 0.021*\"product\" + 0.019*\"amazon\" + 0.014*\"great\" + 0.013*\"box\"\n",
      "\tTopic:1  0.024*\"one\" + 0.021*\"loves\" + 0.018*\"breath\" + 0.016*\"treat\" + 0.016*\"love\"\n"
     ]
    }
   ],
   "source": [
    "topics = 2\n",
    "words = 5\n",
    "get_products(\"B0026RQTGE\", topics, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all three evaluvations for reccomendation system (Final Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine user\n",
    "userId = list(ff_users.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_reccomendations(userId, top_num, pred_type):\n",
    "    #Get user based predicitons for user (user-user similarity)\n",
    "    item_predictions = get_predictions(userId, pred_type)\n",
    "    \n",
    "    #Return item's predictions    \n",
    "    prediction_keys = sorted(item_predictions, key=item_predictions.get, reverse=True)\n",
    "    \n",
    "    #Demote all products that have a negative sentiment below products with positive and negative sentiment reviews\n",
    "    good_product = product_sentiment[product_sentiment['text_label'] == 'pos']\n",
    "    good_product.append(product_sentiment[product_sentiment['text_label'] == 'neu'])\n",
    "    neg_products = [item for item in prediction_keys if item in good_product['productId']]\n",
    "    \n",
    "    prediction_keys = [item for item in prediction_keys if item not in neg_products]\n",
    "    #Add negative products to the end\n",
    "    for i in range(len(neg_products)):\n",
    "        prediction_keys.append(neg_products[i])\n",
    "    \n",
    "    #Print top products with sentiment and topic\n",
    "    print(\"User {}:\\nTop {} reccomendations\".format(userId, top_num))\n",
    "    for i in range(top_num):\n",
    "        curr_item = prediction_keys[i]\n",
    "        full_item = product_sentiment[product_sentiment['productId'] == curr_item]\n",
    "        print(\"  {}. {} similarity:{:.6f} {}:{:.6f}\".format(i+1, curr_item, item_predictions[curr_item], full_item['text_label'].values[0], full_item['text_compound'].values[0]))\n",
    "        get_products(curr_item, 3, 4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User A2A6SB596ROUZA:\n",
      "Top 4 reccomendations\n",
      "  1. B002QWP89S similarity:0.012529 pos:0.694115\n",
      "\tTopic:0  0.025*\"price\" + 0.025*\"product\" + 0.024*\"great\" + 0.021*\"amazon\" + 0.018*\"love\"\n",
      "\tTopic:1  0.026*\"one\" + 0.025*\"greenie\" + 0.019*\"day\" + 0.016*\"treat\" + 0.015*\"loves\"\n",
      "\tTopic:2  0.016*\"get\" + 0.014*\"one\" + 0.014*\"breath\" + 0.014*\"love\" + 0.013*\"treats\"\n",
      "  2. B001EO5Q64 similarity:0.012023 pos:0.814885\n",
      "\tTopic:0  0.025*\"hair\" + 0.024*\"skin\" + 0.013*\"used\" + 0.013*\"also\" + 0.011*\"good\"\n",
      "\tTopic:1  0.019*\"organic\" + 0.019*\"nutiva\" + 0.015*\"good\" + 0.014*\"like\" + 0.014*\"virgin\"\n",
      "\tTopic:2  0.020*\"like\" + 0.016*\"using\" + 0.012*\"would\" + 0.011*\"really\" + 0.010*\"butter\"\n",
      "  3. B002YJ0118 similarity:0.011798 pos:0.753839\n",
      "\tTopic:0  0.026*\"oil\" + 0.022*\"popper\" + 0.020*\"oz\" + 0.018*\"salt\" + 0.011*\"use\"\n",
      "\tTopic:1  0.024*\"best\" + 0.020*\"taste\" + 0.019*\"product\" + 0.019*\"popper\" + 0.017*\"northern\"\n",
      "\tTopic:2  0.023*\"movie\" + 0.022*\"salt\" + 0.017*\"theater\" + 0.015*\"taste\" + 0.014*\"use\"\n",
      "  4. B0026RQTGE similarity:0.010843 pos:0.694115\n",
      "\tTopic:0  0.023*\"size\" + 0.019*\"treats\" + 0.014*\"small\" + 0.013*\"breath\" + 0.012*\"like\"\n",
      "\tTopic:1  0.028*\"amazon\" + 0.028*\"product\" + 0.028*\"price\" + 0.020*\"great\" + 0.016*\"pet\"\n",
      "\tTopic:2  0.030*\"one\" + 0.019*\"loves\" + 0.019*\"greenie\" + 0.018*\"day\" + 0.017*\"breath\"\n"
     ]
    }
   ],
   "source": [
    "get_user_reccomendations(userId, 4, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Positivie Sentiment Scores of Product Reviews:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20d64436910>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "        #print plots of positive and compound values  \n",
    "        print(\"Distribution of Positivie Sentiment Scores of Product Reviews:\")\n",
    "        df[df['productId'] == \"B002QWP89S\"]['text_pos'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
